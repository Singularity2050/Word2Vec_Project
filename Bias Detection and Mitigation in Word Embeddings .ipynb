{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Assignment 1"
      ],
      "metadata": {
        "id": "0HO146OTzjKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview:\n",
        "In this assignment, you will be asked to:\n",
        "\n",
        "  - generate batch for skip-gram model\n",
        "  - implement two loss functions to train word embeddings\n",
        "  - tune the parameters for word embeddings\n",
        "  - apply best learned word embeddings to word analogy task\n",
        "  - calculate bias score on your best models\n",
        "  - create a new task on which you would run WEAT test\n",
        "\n",
        "NOTE:\n",
        "\n",
        "Please only make your code edits in the TODO(students) blocks in the notebook and make sure you have run the previous notebook cells before running the latter ones. Add comments to explain your code well and make sure to use relevant identifier names.\n"
      ],
      "metadata": {
        "id": "VGhklD5PkeOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to use this notebook:\n",
        "  - This notebook is best viewed and executed in Google Colab.\n",
        "  - Please upload the .ipynb version of this notebook in Google Drive on your SBU CS account.\n",
        "  - Double click and select Open with Colab\n",
        "  - Upload the files provided in the current working directory of the Colab notebook"
      ],
      "metadata": {
        "id": "GZOrRLfndVpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please use the following Google Colab Tutorial in case you are not familiar with the tool: [Link](https://colab.research.google.com/drive/16pBJQePbqkz3QFV54L4NIkOn1kwpuRrj)"
      ],
      "metadata": {
        "id": "Pw0rkHABctDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the data and needed libraries"
      ],
      "metadata": {
        "id": "nqOwCznvRnOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download datafile\n",
        "!wget http://mattmahoney.net/dc/text8.zip\n",
        "!unzip text8.zip\n",
        "!rm text8.zipyy"
      ],
      "metadata": {
        "id": "pAsXQn_7Rl8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "E2Z8Npb1U3Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Importing needed libraries and setting up random seeds"
      ],
      "metadata": {
        "id": "6aMOLGZu565p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All import statements\n",
        "\n",
        "import collections\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import math\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Setting up all the seeds for repeatable experiements\n",
        "# DO NOT CHANGE\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)"
      ],
      "metadata": {
        "id": "rpjuG6Tc56H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating the Data\n"
      ],
      "metadata": {
        "id": "h3EdwlSd1d79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train word vectors, you need to generate training instances from the given data. You will implement a method that will generate training instances in batches. For skip-gram model, you will slide a window and sample training instances from the data inside the window.\n",
        "\n",
        "<b>For example:</b>\n",
        "\n",
        "Suppose that we have a text: \"The quick brown fox jumps over the lazy dog.\"\n",
        "and batch_size = 8, window_size = 3\n",
        "\n",
        "\"<font color = red>[The quick brown]</font> fox jumps over the lazy dog\"\n",
        "\n",
        "Context word would be 'quick' and predicting words are 'The' and 'brown'.\n",
        "\n",
        "This will generate training examples of the form context(x), predicted_word(y) like:\n",
        "<ul>\n",
        "      <li>(quick    ,       The)\n",
        "      <li>(quick    ,     brown)\n",
        "</ul>\n",
        "And then move the sliding window.\n",
        "\n",
        "\"The <font color = red>[quick brown fox]</font> jumps over the lazy dog\"\n",
        "\n",
        "In the same way, we have two more examples:\n",
        "<ul>\n",
        "    <li>(brown, quick)\n",
        "    <li>(brown, fox)\n",
        "</ul>\n",
        "\n",
        "Moving the window again:\n",
        "\n",
        "\"The quick <font color = red>[brown fox jumps]</font> over the lazy dog\"\n",
        "\n",
        "We get,\n",
        "\n",
        "<ul>\n",
        "    <li>(fox, brown)\n",
        "    <li>(fox, jumps)\n",
        "</ul>\n",
        "\n",
        "Finally we get two more instances from the moved window,\n",
        "\n",
        "\"The quick brown <font color = red>[fox jumps over]</font> the lazy dog\"\n",
        "\n",
        "<ul>\n",
        "    <li>(jumps, fox)\n",
        "    <li>(jumps, over)\n",
        "</ul>\n",
        "\n",
        "Since now we have 8 training instances, which is the batch size,\n",
        "stop generating this batch and return batch data."
      ],
      "metadata": {
        "id": "GeRFlgId64Xw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two functions given below are helper functions and will assist in feching the data from the file streams."
      ],
      "metadata": {
        "id": "0_HuugPXXuev"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NM90locyL99"
      },
      "outputs": [],
      "source": [
        "# Read the data into a list of strings.\n",
        "def read_data(filename):\n",
        "    with open(filename) as file:\n",
        "        text = file.read()\n",
        "        data = [token.lower() for token in text.strip().split(\" \")]\n",
        "    return data\n",
        "\n",
        "def build_dataset(words, vocab_size):\n",
        "    count = [['UNK', -1]]\n",
        "    count.extend(collections.Counter(words).most_common(vocab_size - 1))\n",
        "    # token_to_id dictionary, id_to_taken reverse_dictionary\n",
        "    vocab_token_to_id = dict()\n",
        "    for word, _ in count:\n",
        "        vocab_token_to_id[word] = len(vocab_token_to_id)\n",
        "    data = list()\n",
        "    unk_count = 0\n",
        "    for word in words:\n",
        "        if word in vocab_token_to_id:\n",
        "            index = vocab_token_to_id[word]\n",
        "        else:\n",
        "            index = 0  # dictionary['UNK']\n",
        "            unk_count += 1\n",
        "        data.append(index)\n",
        "    count[0][1] = unk_count\n",
        "    vocab_id_to_token = dict(zip(vocab_token_to_id.values(), vocab_token_to_id.keys()))\n",
        "    return data, count, vocab_token_to_id, vocab_id_to_token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Variable Description</b>\n",
        "\n",
        "data_index is the index of a word. You can access a word using data[data_index].\n",
        "\n",
        "batch_size is the number of instances in one batch.\n",
        "\n",
        "num_skips is the number of samples you want to draw in a window (in example, it was 2).\n",
        "\n",
        "skip_windows decides how many words to consider left and right from a context word(so, skip_windows*2+1 = window_size).\n",
        "\n",
        "batch will contains word ids for context words. Dimension is [batch_size].\n",
        "\n",
        "labels will contains word ids for predicting words. Dimension is [batch_size, 1].\n"
      ],
      "metadata": {
        "id": "Rn-6QljIlSII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please fill the TODO section in the code below to generate data batches."
      ],
      "metadata": {
        "id": "uh2b0yH2YblN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "    def __init__(self, data, batch_size=128, num_skips=8, skip_window=4):\n",
        "        \"\"\"\n",
        "        @data_index: the index of a word. You can access a word using data[data_index]\n",
        "        @batch_size: the number of instances in one batch\n",
        "        @num_skips: the number of samples you want to draw in a window\n",
        "                (In the below example, it was 2)\n",
        "        @skip_window: decides how many words to consider left and right from a context word.\n",
        "                    (So, skip_windows*2+1 = window_size)\n",
        "        \"\"\"\n",
        "\n",
        "        self.data_index=0\n",
        "        self.data = data\n",
        "        assert batch_size % num_skips == 0\n",
        "        assert num_skips <= 2 * skip_window\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.num_skips = num_skips\n",
        "        self.skip_window = skip_window\n",
        "\n",
        "    def reset_index(self, idx=0):\n",
        "        self.data_index=idx\n",
        "\n",
        "    def generate_batch(self):\n",
        "        \"\"\"\n",
        "        Write the code generate a training batch\n",
        "\n",
        "        batch will contain word ids for context words. Dimension is [batch_size].\n",
        "        labels will contain word ids for predicting(target) words. Dimension is [batch_size, 1].\n",
        "        \"\"\"\n",
        "\n",
        "        center_word = np.ndarray(shape=(self.batch_size), dtype=np.int32) # not initialized\n",
        "        context_word = np.ndarray(shape=(self.batch_size), dtype=np.int32)# not initialized\n",
        "\n",
        "        # stride: for the rolling window\n",
        "        stride = 1\n",
        "\n",
        "        ### TODO(students): start\n",
        "        span = 2 * self.skip_window + 1  # [ skip_window target skip_window ]\n",
        "\n",
        "        buffer = collections.deque(maxlen=span)\n",
        "        for _ in range(span):\n",
        "            buffer.append(self.data[self.data_index])\n",
        "            self.data_index = (self.data_index + 1) % len(self.data)\n",
        "\n",
        "        for i in range(self.batch_size // self.num_skips):\n",
        "            target = self.skip_window  # target label at the center of the buffer\n",
        "            targets_to_avoid = [self.skip_window]\n",
        "            for j in range(self.num_skips):\n",
        "                while target in targets_to_avoid:\n",
        "                    target = random.randint(0, span - 1)\n",
        "                targets_to_avoid.append(target)\n",
        "                center_word[i * self.num_skips + j] = buffer[self.skip_window]\n",
        "                context_word[i * self.num_skips + j] = buffer[target]\n",
        "            buffer.append(self.data[self.data_index])\n",
        "            self.data_index = (self.data_index + 1) % len(self.data)\n",
        "\n",
        "        return torch.LongTensor(center_word), torch.LongTensor(context_word)\n",
        "\n"
      ],
      "metadata": {
        "id": "6AVfTjwS9Q_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Model\n"
      ],
      "metadata": {
        "id": "s_Rl4BwSNibp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<b>Negative Log Likelihood (NLL): </b>\n",
        "We discussed the log likelihood function in class. This is the negative of the same. These are called “loss” functions since they measure how bad the current model is from the expected behavior. Refer to the class notes on this topic.\n",
        "\n",
        "To understand it better, you may also refer to Section 4.3 [here](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf).\n",
        "\n",
        "Training a word2vec model with this loss and the default settings took ~50 mins on Google Colab with GPU accelarator. It will take ~10 hrs on a Macbook Pro 2018 CPU.\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>Negative Sampling (NEG): </b>\n",
        "The negative sampling formulates a slightly different classification task and a corresponding loss.\n",
        "[This paper](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) describes the method in detail.\n",
        "\n",
        "The idea here is to build a classifier that can give high probabilities to words that are the correct target words and low probabilities to words that are incorrect target words.\n",
        "As with negative log likelihood loss, here we define the classifier using a function that uses the word vectors of the context and target as free parameters.\n",
        "The key difference however is that instead of using the entire vocabulary, here we sample a set of k negative words for each instance, and create an augmented instance which is a collection of the true target word and k negative words.\n",
        "Now the vectors are trained to maximize the probability of this augmented instance.\n",
        "To understand it better, you may also refer to Section 4.4 [here](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf).\n",
        "\n",
        "Training a word2vec model with this loss and the default settings took ~2h30 mins on Google Colab with GPU accelarator.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "neDNj2jlmTG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Please implement the models and loss functions in the code below in the TODO sections </b>"
      ],
      "metadata": {
        "id": "mTkFJW1UZd8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the sigmoid function\n",
        "sigmoid = lambda x: 1/(1 + torch.exp(-x))\n",
        "\n",
        "class WordVec(nn.Module):\n",
        "    def __init__(self, V, embedding_dim, loss_func, counts, num_neg_samples_per_center = 1):\n",
        "        super(WordVec, self).__init__()\n",
        "        self.center_embeddings = nn.Embedding(num_embeddings=V, embedding_dim=embedding_dim)\n",
        "        self.center_embeddings.weight.data.normal_(mean=0, std=1/math.sqrt(embedding_dim))\n",
        "        self.center_embeddings.weight.data[self.center_embeddings.weight.data<-1] = -1\n",
        "        self.center_embeddings.weight.data[self.center_embeddings.weight.data>1] = 1\n",
        "\n",
        "        self.context_embeddings = nn.Embedding(num_embeddings=V, embedding_dim=embedding_dim)\n",
        "        self.context_embeddings.weight.data.normal_(mean=0, std=1/math.sqrt(embedding_dim))\n",
        "        self.context_embeddings.weight.data[self.context_embeddings.weight.data<-1] = -1 + 1e-10\n",
        "        self.context_embeddings.weight.data[self.context_embeddings.weight.data>1] = 1 - 1e-10\n",
        "\n",
        "        self.loss_func = loss_func\n",
        "        self.counts = counts\n",
        "\n",
        "        self.num_neg_samples_per_center = num_neg_samples_per_center\n",
        "\n",
        "    def forward(self, center_word, context_word):\n",
        "\n",
        "        if self.loss_func == \"nll\":\n",
        "            return self.negative_log_likelihood_loss(center_word, context_word)\n",
        "        elif self.loss_func == \"neg\":\n",
        "            return self.negative_sampling(center_word, context_word)\n",
        "        else:\n",
        "            raise Exception(\"No implementation found for %s\"%(self.loss_func))\n",
        "\n",
        "    def negative_log_likelihood_loss(self, center_word, context_word):\n",
        "\n",
        "        # Notes (page 9): http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf\n",
        "        center_word_embeddings = self.center_embeddings(center_word) # batches, dims\n",
        "        context_word_embeddings = self.context_embeddings(context_word) # batches, dims\n",
        "\n",
        "        a = torch.sum(torch.mul(center_word_embeddings, context_word_embeddings), axis=1) # batches\n",
        "        # (batches, dims) @ (dims, V) = (batches, V);\n",
        "        b = torch.logsumexp(center_word_embeddings @ self.context_embeddings.weight.T, dim=1) # batches\n",
        "        loss = torch.mean(b - a)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def negative_sampling(self, center_word, context_word):\n",
        "        # Use this variable to control the number of negative samples for every positive sample\n",
        "        num_neg_samples_per_center = self.num_neg_samples_per_center\n",
        "\n",
        "        # Calculate unigram distribution\n",
        "        unigram_dist = np.array(self.counts, dtype=np.int64)\n",
        "        unigram_dist = unigram_dist / np.sum(unigram_dist)\n",
        "        unigram_dist = np.power(unigram_dist, 0.75)\n",
        "        unigram_dist /= np.sum(unigram_dist)\n",
        "\n",
        "        # Sample negative words\n",
        "        batch_size = center_word.size(0)\n",
        "        neg_samples = torch.tensor(\n",
        "            np.random.choice(len(self.counts),\n",
        "                             size=(batch_size, num_neg_samples_per_center),\n",
        "                             replace=True,\n",
        "                             p=unigram_dist\n",
        "                             ),\n",
        "                            dtype=torch.int64).to(center_word.device)\n",
        "\n",
        "        # Calculate the positive term\n",
        "        center_word_embeddings = self.center_embeddings(center_word)\n",
        "        context_word_embeddings = self.context_embeddings(context_word)\n",
        "        pos_term = torch.sum(torch.mul(center_word_embeddings, context_word_embeddings), axis=1)\n",
        "        pos_term = torch.sigmoid(pos_term)\n",
        "        pos_term = torch.log(pos_term)\n",
        "\n",
        "        # Calculate the negative term\n",
        "        neg_word_embeddings = self.context_embeddings(neg_samples)\n",
        "        neg_term = torch.bmm(neg_word_embeddings, center_word_embeddings.unsqueeze(2)).squeeze(2)\n",
        "        neg_term = torch.sigmoid(-neg_term)\n",
        "        neg_term = torch.sum(torch.log(neg_term), axis=1)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = -torch.mean(pos_term + neg_term)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n",
        "    def print_closest(self, validation_words, reverse_dictionary, top_k=8):\n",
        "        print('Printing closest words')\n",
        "        embeddings = torch.zeros(self.center_embeddings.weight.shape).copy_(self.center_embeddings.weight)\n",
        "        embeddings = embeddings.data.cpu().numpy()\n",
        "\n",
        "        validation_ids = validation_words\n",
        "        norm = np.sqrt(np.sum(np.square(embeddings),axis=1,keepdims=True))\n",
        "        normalized_embeddings = embeddings/norm\n",
        "        validation_embeddings = normalized_embeddings[validation_ids]\n",
        "        similarity = np.matmul(validation_embeddings, normalized_embeddings.T)\n",
        "        for i in range(len(validation_ids)):\n",
        "            word = reverse_dictionary[validation_words[i]]\n",
        "            nearest = (-similarity[i, :]).argsort()[1:top_k+1]\n",
        "            print(word, [reverse_dictionary[nearest[k]] for k in range(top_k)])"
      ],
      "metadata": {
        "id": "FFsQcYx2MJfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Data Loading Loops"
      ],
      "metadata": {
        "id": "pJ6wwwGdPk90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below uses the models and losses built above and runs the actual training process."
      ],
      "metadata": {
        "id": "xw3TQl17Zrzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, ckpt_save_path, reverse_dictionary):\n",
        "        self.model = model\n",
        "        self.ckpt_save_path = ckpt_save_path\n",
        "        self.reverse_dictionary = reverse_dictionary\n",
        "\n",
        "    def training_step(self, center_word, context_word):\n",
        "        loss =  self.model(center_word, context_word)\n",
        "        return loss\n",
        "\n",
        "    def train(self, dataset, max_training_steps, ckpt_steps, validation_words, device=\"cpu\", lr = 1):\n",
        "\n",
        "        optim = torch.optim.SGD(self.model.parameters(), lr = lr)\n",
        "        self.model.to(device)\n",
        "        self.model.train()\n",
        "        self.losses = []\n",
        "\n",
        "        t = tqdm(range(max_training_steps))\n",
        "        for curr_step in t:\n",
        "            optim.zero_grad()\n",
        "            center_word, context_word = dataset.generate_batch()\n",
        "            loss = self.training_step(center_word.to(device), context_word.to(device))\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            self.losses.append(loss.item())\n",
        "            if curr_step:\n",
        "                t.set_description(\"Avg loss: %s\"%(round(sum(self.losses[-2000:])/len(self.losses[-2000:]), 3)))\n",
        "            if curr_step % 10000 == 0:\n",
        "                self.model.print_closest(validation_words, self.reverse_dictionary)\n",
        "            if curr_step%ckpt_steps == 0 and curr_step > 0:\n",
        "                self.save_ckpt(curr_step)\n",
        "\n",
        "    def save_ckpt(self, curr_step):\n",
        "        torch.save(self.model, \"%s/%s.pt\"%(self.ckpt_save_path, str(curr_step)))"
      ],
      "metadata": {
        "id": "6_qwLRzKPdiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Framework"
      ],
      "metadata": {
        "id": "Ys10LdRRQp1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following run_training function will train a model for you as shown in the results below. Please use it for the various experiemnts you will perform. The parameters of the run_training() function include many hyperparameters which should be experimented with. Some examples include vector size, batch size, vocabulary size, epochs etc. Please use function call similar to the following cell for guidance on how to train the model."
      ],
      "metadata": {
        "id": "WmU-8MSepfLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_path(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.mkdir(path)\n",
        "        print (\"Created a path: %s\"%(path))\n",
        "\n",
        "def run_training(\n",
        "    model_type = 'nll', # defines which loss function is being used to train the model\n",
        "                        # can take values 'nll' for negative log loss and 'neg' for negative sampling\n",
        "    lr = 0.1, # defines the learning rate used for training the model\n",
        "    num_neg_samples_per_center = 1, # controls the number of negative samples per center word\n",
        "    checkpoint_model_path = './checkpoints', # defines path to the checkpoint of the model\n",
        "    final_model_path = './final_model', # location to save the final model\n",
        "    skip_window = 4, # size of the skip window\n",
        "    vocab_size = int(1e5), # size of the vocabulary used in the experiments\n",
        "    num_skips = 2, # Number of samples to be drawn from a window\n",
        "    batch_size = 64, # Size of the batches in terms of number of x,y pairs used for training\n",
        "    embedding_size = 128, # size of the embedding vectores\n",
        "    checkpoint_step = 50000, # Number of steps after which checkpoint is saved\n",
        "    max_num_steps = 200001 # Maximum number of steps to train for\n",
        "):\n",
        "\n",
        "    checkpoint_model_path = f'{checkpoint_model_path}_{model_type}/'\n",
        "    create_path(checkpoint_model_path)\n",
        "\n",
        "    # Read data\n",
        "    words = read_data(\"./text8\")\n",
        "    print('Data size', len(words))\n",
        "\n",
        "    data, count, vocab_token_to_id, vocab_id_to_token = build_dataset(words, vocab_size)\n",
        "    # save dictionary as vocabulary\n",
        "    print('Most common words (+UNK)', count[:5])\n",
        "    print('Sample data', data[:10], [vocab_id_to_token[i] for i in data[:10]])\n",
        "    # Calculate the probability of unigrams\n",
        "    # unigram_cnt = [c for w, c in count]\n",
        "    count_dict = dict(count)\n",
        "    unigram_cnt = [count_dict[vocab_id_to_token[i]] for i in sorted(list(vocab_token_to_id.values()))]\n",
        "    data_index = 0\n",
        "\n",
        "    dataset = Dataset(data, batch_size=batch_size, num_skips=num_skips, skip_window=skip_window)\n",
        "    center, context = dataset.generate_batch()\n",
        "\n",
        "    for i in range(8):\n",
        "        print(center[i].item(), vocab_id_to_token[center[i].item()],'->', context[i].item(), vocab_id_to_token[context[i].item()])\n",
        "    dataset.reset_index()\n",
        "\n",
        "    valid_size = 16     # Random set of words to evaluate similarity on.\n",
        "    valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
        "    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
        "\n",
        "    embedding_size = embedding_size\n",
        "    model = WordVec(V=vocab_size, embedding_dim=embedding_size, loss_func=model_type, counts=np.array(unigram_cnt), num_neg_samples_per_center = num_neg_samples_per_center)\n",
        "    trainer = Trainer(model, checkpoint_model_path, vocab_id_to_token)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f'Device: {device}')\n",
        "    trainer.train(dataset, max_num_steps, checkpoint_step, valid_examples, device, lr = lr)\n",
        "    model_path = final_model_path\n",
        "    create_path(model_path)\n",
        "    model_filepath = os.path.join(model_path, 'word2vec_%s.model'%(model_type))\n",
        "    pickle.dump([vocab_token_to_id, model.center_embeddings.weight.detach().cpu().numpy()], open(model_filepath, 'wb'))"
      ],
      "metadata": {
        "id": "_EeC-lidQspm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell shows a demo with much lesser training epochs, embedding size and vocabulary size to test your code. Please use values closer to function defaults in the above cell for experimentation in the following sections."
      ],
      "metadata": {
        "id": "Yg9Hobhyqja_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please make sure to keep an eye on the Avg. Loss value being printed as the model trains. This value sgould gradually decrease if you have implemented your code well."
      ],
      "metadata": {
        "id": "DtGdO0drYoOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_training(\n",
        "    model_type = 'neg', # defines which loss function is being used to train the model\n",
        "                        # can take values 'nll' for negative log loss and 'neg' for negative sampling\n",
        "    lr = 0.1, # defines the learning rate used for training the model\n",
        "    num_neg_samples_per_center = 5, # controls the number of negative samples per center word\n",
        "    checkpoint_model_path = './demo_checkpoints', # defines path to the checkpoint of the model\n",
        "    final_model_path = './final_demo_model', # location to save the final model\n",
        "    skip_window = 4, # size of the skip window\n",
        "    vocab_size = int(1e5), # size of the vocabulary used in the experiments\n",
        "    num_skips = 8, # Number of samples to be drawn from a window\n",
        "    batch_size = 256, # Size of the batches in terms of number of x,y pairs used for training\n",
        "    embedding_size = 4, # size of the embedding vectores\n",
        "    checkpoint_step = 500, # Number of steps after which checkpoint is saved\n",
        "    max_num_steps = 2001 # Maximum number of steps to train for\n",
        ")"
      ],
      "metadata": {
        "id": "aCOcE0Uqej4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Please train at least one model of each NLL and NEG using the above function. Make sure you name the files and checkpoints appropriately.</b>"
      ],
      "metadata": {
        "id": "mLY01IbAeww5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO(student): start\n",
        "\n",
        "# train at least 1 NEG model\n",
        "\n",
        "# train at least 1 NLL model\n",
        "\n",
        "# TODO(student): end"
      ],
      "metadata": {
        "id": "BXSBmxqpEVcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Framework"
      ],
      "metadata": {
        "id": "5KxsyQ8gsF-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Analogies using word vectors</b>\n",
        "\n",
        "You will use the word vectors you learned from both approaches in the following word analogy task.\n",
        "\n",
        "Each question/task is in the following form.\n",
        "```\n",
        "Consider the following word pairs that share the same relation, R:\n",
        "\n",
        "    pilgrim:shrine, hunter:quarry, assassin:victim, climber:peak\n",
        "\n",
        "Among these word pairs,\n",
        "\n",
        "(1) pig:mud\n",
        "(2) politician:votes\n",
        "(3) dog:bone\n",
        "(4) bird:worm\n",
        "\n",
        "Q1. Which word pairs has the MOST illustrative(similar) example of the relation R?\n",
        "\n",
        "The word pair that has the most illustrative example of the relation R is pilgrim:shrine,\n",
        "because it implies that a pilgrim visits a shrine, just as a hunter visits a quarry,\n",
        "an assassin targets a victim, and a climber seeks to reach a peak.\n",
        "\n",
        "Q2. Which word pairs has the LEAST illustrative(similar) example of the relation R?\n",
        "\n",
        "The word pair that has the LEAST illustrative example of the relation R is bird:worm,\n",
        "because while it implies that a bird eats a worm,\n",
        "it is not as clear or explicit in demonstrating the same type of direct relationship as the other pairs.\n",
        "```\n",
        "\n",
        "For each question, there are examples pairs of a certain relation. Your task is to find the most/least illustrative word pair of the relation. One simple method to answer those questions will be measuring the similarities of difference vectors.\n",
        "\n",
        "Recall that vectors are representing some direction in space. If (a, b) and (c, d) pairs are analogous pairs then the transformation from a to b (i.e., some x vector when added to a gives b: a + x = b) should be highly similar to the transformation from c to d (i.e., some y vector when added to c gives d: c + y = d). In other words, the difference vector (b-a) should be similar to difference vector (d-c).\n",
        "\n",
        "This difference vector can be thought to represent the relation between the two words.\n",
        "\n",
        "<b>Please fill-in the TODO section below to implement the above mentioned task.</b>\n",
        "\n",
        "Due to the noisy annotation data, the expected accuracy is not high. The NLL default overall accuracy is 33.5% and negative sampling default overall accuracy is 33.6%.\n",
        "Improving this score 1~3% would be your goal.\n"
      ],
      "metadata": {
        "id": "zswvfdgesjeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Further implementation instructions:</b>\n",
        "\n",
        "  - `In the next 2 cells`:\n",
        "    You will write a code in the TODO section for evaluating relation between pairs of words -- called the [MaxDiff question](https://en.wikipedia.org/wiki/MaxDiff).\n",
        "    You will generate a file with your predictions following the format of `word_analogy_sample_predictions.txt`.\n",
        "\n",
        "  - `evaluate_word_analogy.pl`:\n",
        "    This is a perl script to evaluate YOUR PREDICTIONS on development data. Use it as shown in the next cell. You do not need to submit prediction or score files related to the dev set.\n",
        "\n",
        "  - `word_analogy_dev.txt`:\n",
        "    This is some data for development.\n",
        "    Each line of this file is divided into \"examples\" and \"choices\" by \"||\".\n",
        "        [examples]||[choices]\n",
        "    \"Examples\" and \"choices\" are delimited by a comma.\n",
        "      For example:  \"tailor:suit\",\"oracle:prophesy\",\"baker:flour\"\n",
        "\n",
        "  - `word_analogy_dev_sample_predictions.txt`:\n",
        "    A sample prediction file. Pay attention to the format of this file.\n",
        "    Your prediction file should follow this to use \"score_maxdiff.pl\" script.\n",
        "    Each row is in this format:\n",
        "    \n",
        "      <pair1> <pair2> <pair3> <pair4> <least_illustrative_pair> <most_illustrative_pair>\n",
        "\n",
        "    The order of word pairs should match their original order found in `word_analogy_dev.txt`.\n",
        "\n",
        "  - `word_analogy_dev_mturk_answers.txt`:\n",
        "    This is the answers collected using Amazon mechanical turk for `word_analogy_dev.txt`.\n",
        "    The answers in this file is used as the correct answer and used to evaluate your analogy predictions. (using \"evaluate_word_analogy.pl\")\n",
        "    For your information, the answers here are a little bit noisy.\n",
        "\n",
        "  - `word_analogy_test.txt`:\n",
        "    Test data file. When you are done experimenting with your model, you will generate predictions for this test data using your best models (NLL/negative sampling).\n",
        "    You will not be able to run the evaluation script on the test set.\n",
        "\n",
        "    Make sure your submission files are named: `test_preds_nll.txt`, `test_preds_neg.txt`.\n"
      ],
      "metadata": {
        "id": "iUD-elYkPi_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(file_path):\n",
        "    with open(file_path,'r') as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "    candidate, test = [], []\n",
        "    for line in data:\n",
        "        a, b = line.strip().split(\"||\")\n",
        "        a = [i[1:-1].split(\":\") for i in a.split(\",\")]\n",
        "        b = [i[1:-1].split(\":\") for i in b.split(\",\")]\n",
        "        candidate.append(a)\n",
        "        test.append(b)\n",
        "\n",
        "    return candidate, test\n",
        "\n",
        "def get_embeddings(examples, embeddings, dictionary):\n",
        "\n",
        "    \"\"\"\n",
        "    For the word pairs in the 'examples' array, fetch embeddings and return.\n",
        "    You can access your trained model via dictionary and embeddings.\n",
        "    dictionary[word] will give you word_id\n",
        "    and embeddings[word_id] will return the embedding for that word.\n",
        "\n",
        "    word_id = dictionary[word]\n",
        "    v1 = embeddings[word_id]\n",
        "\n",
        "    or simply\n",
        "\n",
        "    v1 = embeddings[dictionary[word_id]]\n",
        "    \"\"\"\n",
        "\n",
        "    norm = np.sqrt(np.sum(np.square(embeddings),axis=1,keepdims=True))\n",
        "    normalized_embeddings = embeddings/norm\n",
        "\n",
        "    embs = []\n",
        "    for line in examples:\n",
        "        temp = []\n",
        "        for pairs in line:\n",
        "            temp.append([ normalized_embeddings[dictionary[pairs[0]]], normalized_embeddings[dictionary[pairs[1]]] ])\n",
        "        embs.append(temp)\n",
        "\n",
        "    result = np.array(embs)\n",
        "\n",
        "    return result\n",
        "\n",
        "def evaluate_pairs(candidate_embs, test_embs):\n",
        "\n",
        "    \"\"\"\n",
        "    Write code to evaluate a relation between pairs of words.\n",
        "    Find the best and worst pairs and return that.\n",
        "    \"\"\"\n",
        "\n",
        "    best_pairs = []\n",
        "    worst_pairs = []\n",
        "\n",
        "    ### TODO(students): start\n",
        "    for line_candidate_embs, line_test_embs in zip(candidate_embs, test_embs):\n",
        "      similarities = []\n",
        "      for candidate_emb_pair, test_emb_pair in zip(line_candidate_embs, line_test_embs):\n",
        "          similarity = np.dot(candidate_emb_pair[0], candidate_emb_pair[1]) * np.dot(test_emb_pair[0], test_emb_pair[1])\n",
        "          similarities.append(similarity)\n",
        "\n",
        "      best_pair_index = np.argmax(similarities)\n",
        "      worst_pair_index = np.argmin(similarities)\n",
        "\n",
        "      best_pairs.append(best_pair_index)\n",
        "      worst_pairs.append(worst_pair_index)\n",
        "    ### TODO(students): end\n",
        "\n",
        "    return best_pairs, worst_pairs\n",
        "\n",
        "def write_solution(best_pairs, worst_pairs, test, path):\n",
        "\n",
        "    \"\"\"\n",
        "    Write best and worst pairs to a file, that can be evaluated by evaluate_word_analogy.pl\n",
        "    \"\"\"\n",
        "\n",
        "    ans = []\n",
        "    for i, line in enumerate(test):\n",
        "        temp = [f'\"{pairs[0]}:{pairs[1]}\"' for pairs in line]\n",
        "        temp.append(f'\"{line[worst_pairs[i]][0]}:{line[worst_pairs[i]][1]}\"')\n",
        "        temp.append(f'\"{line[best_pairs[i]][0]}:{line[best_pairs[i]][1]}\"')\n",
        "        ans.append(\" \".join(temp))\n",
        "\n",
        "    with open(path, 'w') as f:\n",
        "        f.write(\"\\n\".join(ans))\n",
        "\n",
        "\n",
        "def run_word_analogy_eval(\n",
        "    model_path = './final_model', # path to the location where the model being evaluated is stored\n",
        "    input_filepath = 'word_analogy_dev.txt', # Word analogy file to evaluate on\n",
        "    output_filepath = 'word_analogy_demo_results.txt', # predicted results\n",
        "    model_type = 'nll' # type of model being used, NLL or NEG\n",
        "):\n",
        "\n",
        "    print(f'Model file: {model_path}/word2vec_{model_type}.model')\n",
        "    model_filepath = os.path.join(model_path, 'word2vec_%s.model'%(model_type))\n",
        "\n",
        "    dictionary, embeddings = pickle.load(open(model_filepath, 'rb'))\n",
        "\n",
        "    candidate, test = read_data(input_filepath)\n",
        "\n",
        "    candidate_embs = get_embeddings(candidate, embeddings, dictionary)\n",
        "    test_embs = get_embeddings(test, embeddings, dictionary)\n",
        "\n",
        "    best_pairs, worst_pairs = evaluate_pairs(candidate_embs, test_embs)\n",
        "\n",
        "    out_filepath = output_filepath\n",
        "    print(f'Output file: {out_filepath}')\n",
        "    write_solution(best_pairs, worst_pairs, test, out_filepath)"
      ],
      "metadata": {
        "id": "Xg-sE4hfsJxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the word_analogy code is complete with the TODO section, you can use the following function call to generate the results of the word analogy task. A demo result set is also provided for reference."
      ],
      "metadata": {
        "id": "estUQuAXL6bY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_word_analogy_eval(\n",
        "    model_path = './final_demo_model', # path to the location where the model being evaluated is stored\n",
        "    input_filepath = 'word_analogy_dev.txt', # Word analogy file to evaluate on\n",
        "    output_filepath = 'word_analogy_demo_dev_results.txt', # predicted results\n",
        "    model_type = 'neg' # type of model being used, NLL or NEG\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "46Rzv8Q6wVS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results can finally be converted into numeric metrics using the Perl script below. A demo score result is also provided for refernce."
      ],
      "metadata": {
        "id": "oXBKXZvcMVN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 evaluate_word_analogy.pl\n",
        "!./evaluate_word_analogy.pl word_analogy_dev_mturk_answers.txt word_analogy_demo_dev_results.txt demo_score_neg.txt"
      ],
      "metadata": {
        "id": "jLBe5k_DGs1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Experiments with various hyper-parameters for the Neg models"
      ],
      "metadata": {
        "id": "UqgKOnZ7Qg8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should run five experiments, where each experiment involves learning word vectors using Negative Sampling model with a specific setting for the three hyper parameters listed below AND evaluating the resulting word vectors on the test set of the word analogy task.\n",
        "\n",
        "Hyper parameters to try:\n",
        "  - Number of Neg samples (Can vary from 1 to 5)\n",
        "  - Learning Rate (Can vary from 0.1 to 10)\n",
        "  - Window size (Can vary from 1 to 10)\n",
        "\n",
        "For example one experiment can be where you train a model with number of negative samples set to 5, the learning rate set to 6.4, and window size set to 7. With this you will get a set of word vectors, which you will evaluate on the test set for the word analogy task.\n",
        "\n",
        "For each experiment you will record your guess for what you think will happen, your recording of what happened, and a guess for an explanation of why it happened. You are doing these experiments to see what happens. Your guesses for what should happen or the explanations themselves wont be graded."
      ],
      "metadata": {
        "id": "3Kf1fWWnYH6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Please make sure that you also test your code on word_analogy_test.txt </b>"
      ],
      "metadata": {
        "id": "lbv-GF8JYdB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 1"
      ],
      "metadata": {
        "id": "7m0Q604fnADQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> What are the hyperparameters selected for this experiment and why did you select them?</b>"
      ],
      "metadata": {
        "id": "qXyy6egInJA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters: Number of Neg samples = 5, Learning Rate = 1, Window size = 5.\n",
        "\n",
        "I selected these hyperparameters as a starting point since they are the default values used in the original code. I want to see how the model performs with these default values and to use it as a baseline to compare against other experiments."
      ],
      "metadata": {
        "id": "ACsHlAnDnj2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> How do you expect the model to behave? </b>"
      ],
      "metadata": {
        "id": "xfkr5o1ln3bU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the chosen hyperparameters, I expect the model to exhibit a decent performance, as these settings have been commonly used and proven to work well in many scenarios. However, this does not guarantee the best possible results, and there might be room for improvement through hyperparameter tuning. Using these default values will allow us to establish a baseline from which we can refine the model by adjusting the hyperparameters based on the specific dataset and task at hand."
      ],
      "metadata": {
        "id": "6a0ui6zEneGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO(students): start\n",
        "\n",
        "## Train a new model with the suggested hyperparameters and run the word analogy test\n",
        "\n",
        "## TODO(students): end"
      ],
      "metadata": {
        "id": "6DIPVJYFQYMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> What did you observe? Do the observations follow your expectations? Give a plausible cause.</b>"
      ],
      "metadata": {
        "id": "211TegT0ndEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Generated by:                                     score_maxdiff.pl\n",
        "Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt\n",
        "Test File:                                        word_analogy_demo_results.txt\n",
        "Number of MaxDiff Questions:                      914\n",
        "Number of Least Illustrative Guessed Correctly:   296\n",
        "Number of Least Illustrative Guessed Incorrectly: 618\n",
        "Accuracy of Least Illustrative Guesses:            32.4%\n",
        "Number of Most Illustrative Guessed Correctly:    307\n",
        "Number of Most Illustrative Guessed Incorrectly:  607\n",
        "Accuracy of Most Illustrative Guesses:             33.6%\n",
        "Overall Accuracy:                                  33.0%\n",
        "\n",
        "\n",
        "Based on the chosen hyperparameters and the model's performance, we observed unexpected outcomes. It seems that the overall accuracy of the model is relatively low, at 33.0%. The accuracy of guessing the least illustrative words is 32.4%, and the accuracy of guessing the most illustrative words is 33.6%. These observations do not indicate a significant improvement in the model's performance compared to the default hyperparameters.\n",
        "\n",
        "Some plausible causes for unexpected observations could include:\n",
        "\n",
        "1. Insufficient training data: If the model has not been exposed to a diverse and representative dataset, it might not generalize well to new data.\n",
        "\n",
        "\n",
        "2. Overfitting or underfitting: Overfitting occurs when the model learns the training data too well, including noise, while underfitting happens when the model fails to capture the underlying patterns in the data. Both situations can lead to poor performance on unseen data.\n",
        "\n",
        "\n",
        "3. Suboptimal hyperparameters: The chosen hyperparameters might not be the best fit for the dataset or task. Experimenting with different hyperparameter values can help find a better configuration that results in improved performance."
      ],
      "metadata": {
        "id": "v1Os5TuVNdth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 2"
      ],
      "metadata": {
        "id": "qljvA7oeNjQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> What are the hyperparameters selected for this experiment and why did you select them?</b>"
      ],
      "metadata": {
        "id": "rG_kBj2HNjQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters: Number of Neg samples = 5, Learning Rate = 1, Window size = 9.\n",
        "\n",
        "I selected these alternative hyperparameters to explore how different configurations affect the performance of the Word2Vec model. Changing the hyperparameters allows us to investigate how each change impacts the model's ability to capture semantic and syntactic relationships between words. The rationale for each hyperparameter choice is as follows:\n",
        "\n",
        "Window Size = 9: By increasing the window size, the model can capture a larger context for the target word, allowing it to learn more meaningful relationships between words and better understand their semantic meanings. A larger window size provides more context, which can be beneficial for learning, but it also increases the computational cost of training the model."
      ],
      "metadata": {
        "id": "Q84Jr57_NjQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> How do you expect the model to behave? </b>"
      ],
      "metadata": {
        "id": "mlX5d8K-NjQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the alternative hyperparameters chosen (Number of Negative Samples = 5, Learning Rate = 1, and Window Size = 9), I expect the model to behave differently compared to the default settings.\n",
        "\n",
        "Increasing the window size to 9 allows the model to consider a broader context when learning word representations. This could potentially lead to better capture of long-range dependencies between words, and therefore, improved performance on tasks that require understanding of broader context."
      ],
      "metadata": {
        "id": "KDlO6nCDNjQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO(students): start\n",
        "\n",
        "## Train a new model with the suggested hyperparameters and run the word analogy test\n",
        "\n",
        "## TODO(students): end"
      ],
      "metadata": {
        "id": "BcO2smirNjQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> What did you observe? Do the observations follow your expectations? Give a plausible cause.</b>"
      ],
      "metadata": {
        "id": "N7E9-wF5NjQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Result: Generated by:                                     score_maxdiff.pl\n",
        "Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt\n",
        "Test File:                                        word_analogy_demo_dev_results.txt\n",
        "Number of MaxDiff Questions:                      914\n",
        "Number of Least Illustrative Guessed Correctly:   287\n",
        "Number of Least Illustrative Guessed Incorrectly: 627\n",
        "Accuracy of Least Illustrative Guesses:            31.4%\n",
        "Number of Most Illustrative Guessed Correctly:    337\n",
        "Number of Most Illustrative Guessed Incorrectly:  577\n",
        "Accuracy of Most Illustrative Guesses:             36.9%\n",
        "Overall Accuracy:                                  34.1%\n",
        "\n",
        "\n",
        "The results show that the overall accuracy of the model with the alternative hyperparameters (Number of Negative Samples = 5, Learning Rate = 1, and Window Size = 9) is 34.1%. This is a slight improvement compared to the default settings, where the overall accuracy was 33.6%.\n",
        "\n",
        "The observations partially follow the expectations. The increased window size likely allowed the model to better capture long-range dependencies between words, leading to a slight improvement in the overall accuracy. However, the improvement is not substantial, which could be attributed to the high learning rate of 1. As mentioned earlier, a high learning rate can lead to less stable learning and difficulty in converging to an optimal solution.\n",
        "\n",
        "The plausible cause for the observed improvement in performance is the increased window size, which allowed the model to consider a broader context when learning word representations. However, the high learning rate might have limited the extent of the improvement by causing less stable learning."
      ],
      "metadata": {
        "id": "DolvZGynNjQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 3"
      ],
      "metadata": {
        "id": "iBVrNskUNnCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> What are the hyperparameters selected for this experiment and why did you select them?</b>"
      ],
      "metadata": {
        "id": "ssKIwG8vNnCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters: Number of Neg samples = 1, Learning Rate = 1, Window size = 5.\n",
        "In this experiment, the selected hyperparameters are:\n",
        "\n",
        "Number of Negative Samples = 1: By reducing the number of negative samples, the model will now have fewer negative examples to compare against the positive example for each center word. This change might result in faster training but could also lead to a less robust representation, as the model is exposed to fewer contrasting examples.\n",
        "\n",
        "These hyperparameters were selected to investigate the impact of reducing the number of negative samples while keeping the other parameters at their default values."
      ],
      "metadata": {
        "id": "F1LDDxfPNnCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> How do you expect the model to behave? </b>"
      ],
      "metadata": {
        "id": "kOkBZCeGNnCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the number of negative samples has been reduced, the model may train faster due to having fewer contrasting examples to process. However, this could lead to a less robust representation and possibly lower performance, as the model has less exposure to various negative examples."
      ],
      "metadata": {
        "id": "On5SBOizNnCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO(students): start\n",
        "\n",
        "## Train a new model with the suggested hyperparameters and run the word analogy test\n",
        "\n",
        "## TODO(students): end"
      ],
      "metadata": {
        "id": "JDIqINOlNnCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> What did you observe? Do the observations follow your expectations? Give a plausible cause.</b>"
      ],
      "metadata": {
        "id": "OODYQp28NnCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Result: Generated by:                                     score_maxdiff.pl\n",
        "Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt\n",
        "Test File:                                        word_analogy_demo_dev_results.txt\n",
        "Number of MaxDiff Questions:                      914\n",
        "Number of Least Illustrative Guessed Correctly:   286\n",
        "Number of Least Illustrative Guessed Incorrectly: 628\n",
        "Accuracy of Least Illustrative Guesses:            31.3%\n",
        "Number of Most Illustrative Guessed Correctly:    341\n",
        "Number of Most Illustrative Guessed Incorrectly:  573\n",
        "Accuracy of Most Illustrative Guesses:             37.3%\n",
        "Overall Accuracy:                                  34.3%\n",
        "\n",
        "Based on the results obtained, the overall accuracy is 34.3%, which is slightly higher than the default settings but still not significantly different. The accuracy of least illustrative guesses is 31.3%, and the accuracy of most illustrative guesses is 37.3%.\n",
        "\n",
        "The observations partially follow the expectations. As expected, the model trained faster due to the reduction in the number of negative samples, but the overall performance did not improve significantly. The plausible cause for this observation could be that the reduction in the number of negative samples had a limited impact on the model's ability to learn a more robust representation. With fewer contrasting examples, the model may not have been able to capture the full complexity of the relationships between words.\n",
        "\n",
        "However, since the overall accuracy did not decrease substantially, this suggests that the reduction in negative samples did not have a detrimental effect on the model's performance. The model could still learn meaningful relationships between words, although the overall improvement was minimal.\n"
      ],
      "metadata": {
        "id": "ef-4RoZ4NnCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 4"
      ],
      "metadata": {
        "id": "-F9QRU4sNvpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> What are the hyperparameters selected for this experiment and why did you select them?</b>"
      ],
      "metadata": {
        "id": "UAmurwfDNvpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters: Number of Neg samples = 1, Learning Rate = 0.1, Window size = 9.\n",
        "\n",
        "I chose these hyperparameters to explore the effect of reducing the number of negative samples while also decreasing the learning rate and increasing the window size. The lower learning rate is expected to make the model learn more slowly, allowing for more fine-grained adjustments to the weights. The increased window size allows the model to consider a broader context while learning word relationships. By adjusting these hyperparameters, I aim to investigate the impact of these changes on the model's performance and gain insights into the model's behavior under different conditions."
      ],
      "metadata": {
        "id": "kPlyw0cgNvpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> How do you expect the model to behave? </b>"
      ],
      "metadata": {
        "id": "cwisiH0VNvpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I expect the lower number of negative samples to make the model less effective at distinguishing between positive and negative relationships, but it will also make the training process faster.\n",
        "\n",
        "The reduced learning rate should make the model's learning process slower and more gradual. This can lead to better convergence and a more fine-grained understanding of the relationships between words, although it might also require more training iterations to reach optimal performance.\n",
        "\n",
        "The increased window size allows the model to consider a larger context when learning word relationships. This can help the model to capture more complex associations and dependencies between words, potentially leading to a better understanding of the word relationships and improved overall performance.\n"
      ],
      "metadata": {
        "id": "q4wAvUweNvpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO(students): start\n",
        "\n",
        "## Train a new model with the suggested hyperparameters and run the word analogy test\n",
        "\n",
        "## TODO(students): end"
      ],
      "metadata": {
        "id": "LSI_XMtFNvpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> What did you observe? Do the observations follow your expectations? Give a plausible cause.</b>"
      ],
      "metadata": {
        "id": "faG0h-dpNvpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "result :\n",
        "Generated by:                                     score_maxdiff.pl\n",
        "Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt\n",
        "Test File:                                        word_analogy_demo_dev_results.txt\n",
        "Number of MaxDiff Questions:                      914\n",
        "Number of Least Illustrative Guessed Correctly:   286\n",
        "Number of Least Illustrative Guessed Incorrectly: 628\n",
        "Accuracy of Least Illustrative Guesses:            31.3%\n",
        "Number of Most Illustrative Guessed Correctly:    342\n",
        "Number of Most Illustrative Guessed Incorrectly:  572\n",
        "Accuracy of Most Illustrative Guesses:             37.4%\n",
        "Overall Accuracy:                                  34.4%\n",
        "\n",
        "\n",
        "The results show that the overall accuracy of the model with the selected hyperparameters (Number of Negative Samples = 1, Learning Rate = 0.1, and Window Size = 9) is 34.4%. The accuracy of the least illustrative guesses is 31.3%, while the accuracy of the most illustrative guesses is 37.4%.\n",
        "\n",
        "These results are slightly better than the default settings but not by a significant margin. This might be due to the trade-offs involved in selecting the hyperparameters. The lower number of negative samples and reduced learning rate might have helped improve the model's learning process, but the benefits could have been offset by the slower convergence and the need for more training iterations.\n",
        "\n",
        "The increased window size might have helped the model to capture more complex associations and dependencies between words. However, it is possible that the chosen dataset does not contain enough examples of such complex relationships to significantly improve the model's performance.\n",
        "\n",
        "Overall, the observations are in line with the expectations to some extent, but they also highlight the challenges involved in fine-tuning hyperparameters for optimal performance. Further experimentation with different combinations of hyperparameters and larger datasets may be needed to achieve more significant improvements in the model's performance.\n"
      ],
      "metadata": {
        "id": "18UrXMUyNvpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 5"
      ],
      "metadata": {
        "id": "0FhkA0iNN0Zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> What are the hyperparameters selected for this experiment and why did you select them?</b>"
      ],
      "metadata": {
        "id": "fdl0h3UdN0Zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters: Number of Neg samples = 5, Learning Rate = 0.1, Window size = 9.\n",
        "\n",
        "The selected hyperparameters were chosen to explore the impact of different values on the model's performance. The reasoning behind each selection is as follows:\n",
        "\n",
        "Number of Negative Samples = 5: Increasing the number of negative samples can improve the model's ability to learn meaningful relationships between words by providing more negative examples for comparison. However, it also increases the training complexity. By selecting a moderate number of negative samples, such as 5, we aim to balance the benefits of having more negative samples with the computational cost.\n",
        "\n",
        "Learning Rate = 0.1: Lowering the learning rate from the default value helps prevent overshooting the optimal solution during the gradient descent optimization process. A lower learning rate can lead to more stable convergence and potentially better model performance. However, it may also slow down the training process, so a balance between convergence speed and stability is desired.\n",
        "\n",
        "Window Size = 9: By increasing the window size, the model can capture a larger context for the target word, allowing it to learn more meaningful relationships between words and better understand their semantic meanings. A larger window size provides more context, which can be beneficial for learning, but it also increases the computational cost of training the model.\n",
        "\n",
        "The selected hyperparameters aim to strike a balance between improving the model's ability to learn meaningful word relationships and managing the computational cost of training. These values can be further fine-tuned based on the model's performance to optimize the results."
      ],
      "metadata": {
        "id": "EWNU2PYTN0Zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> How do you expect the model to behave? </b>"
      ],
      "metadata": {
        "id": "ld8yH3bLN0Zy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "we expect the model to learn meaningful word relationships and have a better understanding of the semantic meanings of words in its vocabulary. It may take longer to train due to the lower learning rate, but the results could be more stable and accurate. The larger window size should help capture more context, while the moderate number of negative samples should provide a good balance between performance and computational cost."
      ],
      "metadata": {
        "id": "OEOh1RfPN0Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO(students): start\n",
        "\n",
        "## Train a new model with the suggested hyperparameters and run the word analogy test\n",
        "\n",
        "## TODO(students): end"
      ],
      "metadata": {
        "id": "6IJSUIn7N0Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> What did you observe? Do the observations follow your expectations? Give a plausible cause.</b>"
      ],
      "metadata": {
        "id": "mc7WuBtTN0Zy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Result : Generated by:                                     score_maxdiff.pl\n",
        "Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt\n",
        "Test File:                                        word_analogy_demo_dev_results.txt\n",
        "Number of MaxDiff Questions:                      914\n",
        "Number of Least Illustrative Guessed Correctly:   286\n",
        "Number of Least Illustrative Guessed Incorrectly: 628\n",
        "Accuracy of Least Illustrative Guesses:            31.3%\n",
        "Number of Most Illustrative Guessed Correctly:    342\n",
        "Number of Most Illustrative Guessed Incorrectly:  572\n",
        "Accuracy of Most Illustrative Guesses:             37.4%\n",
        "Overall Accuracy:                                  34.4%\n",
        "\n",
        "Based on the results you provided, the overall accuracy of the model is 34.4%. This is a slight improvement over the default overall accuracy of 33.6%. The observations show that the model performed slightly better when you lowered the learning rate and increased the number of negative samples and window size.\n",
        "\n",
        "A plausible cause for this improvement could be that the increased window size allowed the model to consider a broader context while learning word representations. Additionally, using more negative samples might have contributed to better training of the model by providing a more diverse set of negative samples for the model to learn from. Finally, lowering the learning rate might have helped the model to converge more smoothly during training, potentially avoiding large weight updates that could lead to unstable learning."
      ],
      "metadata": {
        "id": "zwISJiMiN0Zz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the Negative Log-likelihood (NLL) method.\n",
        "\n",
        "Learn word vectors using the negative log-likelihood method with the same settings of hyper parameters as in your Experiment 1 above. (Note that number of negative samples does not apply in this case). Test the resulting vectors on the test set of the word analogy task.\n",
        "<br/>\n",
        "\n",
        "\n",
        "<b>What do you observe? How much is the model accuracy? How long did it take for you to train?</b>\n",
        "\n",
        "Result:\n",
        "Generated by:                                     score_maxdiff.pl\n",
        "Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt\n",
        "Test File:                                        word_analogy_demo_dev_results.txt\n",
        "Number of MaxDiff Questions:                      914\n",
        "Number of Least Illustrative Guessed Correctly:   277\n",
        "Number of Least Illustrative Guessed Incorrectly: 637\n",
        "Accuracy of Least Illustrative Guesses:            30.3%\n",
        "Number of Most Illustrative Guessed Correctly:    332\n",
        "Number of Most Illustrative Guessed Incorrectly:  582\n",
        "Accuracy of Most Illustrative Guesses:             36.3%\n",
        "Overall Accuracy:                                  33.3%\n",
        "\n",
        "Upon running the Negative Log-likelihood (NLL) method with the same hyperparameter settings as in Experiment 1 (Learning Rate = 1 and Window Size = 5), the following observations were made:\n",
        "\n",
        "Overall accuracy: 33.3%\n",
        "Accuracy of least illustrative guesses: 30.3%\n",
        "Accuracy of most illustrative guesses: 36.3%\n",
        "These results indicate that the model has a moderate performance on the word analogy task with the given hyperparameters. The training time for this specific experiment was not provided, but it can be expected to vary depending on the hardware and the size of the dataset used.\n"
      ],
      "metadata": {
        "id": "-y_UyAKpdY7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclude the results of your experiments. Include a table in the notebook showing all your results."
      ],
      "metadata": {
        "id": "Q5McV-znN9K1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of hyperparameters has a significant impact on the performance of the model.\n",
        "Both negative sampling and negative log-likelihood methods are affected by the hyperparameter settings.\n",
        "There is no single optimal configuration of hyperparameters, as different settings might work better for different tasks and datasets.\n",
        "\n",
        "It is essential to note that these experiments were conducted using specific datasets and tasks, and the results might vary when applied to other tasks or datasets. Furthermore, more exhaustive hyperparameter tuning and optimization could potentially improve model performance."
      ],
      "metadata": {
        "id": "sWOjjGMsN746"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WEAT Test"
      ],
      "metadata": {
        "id": "sxBHHqj_Oz_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have seen the power of word embeddings to help learn analogies, so it is also appropriate to show the unwanted learnings of the generated embeddings.\n",
        "In this task, we will be looking at how to evaluate whether the embeddings are biased or not.\n",
        "\n",
        "The WEAT test provides a way to measure quantifiably the bias in the word embeddings. [This paper](https://arxiv.org/pdf/1810.03611.pdf) describes the method in detail.\n",
        "\n",
        "The basic idea is to examine the associations in word embeddings between concepts.\n",
        "It measures the degree to which a model associates sets of target words (e.g., African American names, European American names, flowers, insects) with sets of attribute words (e.g., ”stable”, ”pleasant” or ”unpleasant”).\n",
        "The association between two given words is defined as the cosine similarity between the embedding vectors for the words.\n"
      ],
      "metadata": {
        "id": "tbPr6A28O2FO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will generate the bias scores as evaluated on 5 different tasks with different sets of attributes (A and B) and targets (X and Y) as defined in the file pointed to in the `weat_file_path` (`weat.json` for the given data). This will print and dump the output in the filepath you specify.\n",
        "\n",
        "For task addition for WEAT Test:\n",
        "Follow the task definition as done for the other WEAT tasks to add custom task of your own!.\n",
        "\n",
        "Add to the json file `custom_weat.json`, another task in the following format:\n",
        "```\n",
        "{\n",
        "  # initial tasks....\n",
        "  \"custom_task\": {\n",
        "    \"A_key\": \"A_val\",\n",
        "    \"B_key\": \"B_val\",\n",
        "    \"X_key\": \"X_val\",\n",
        "    \"Y_key\": \"Y_val\",\n",
        "    \"A_val\": [\n",
        "      # list of words for attribute A\n",
        "    ],\n",
        "    \"B_val\": [\n",
        "      # list of words for attribute B\n",
        "    ],\n",
        "    \"X_val\": [\n",
        "      # list of words for target X\n",
        "    ],\n",
        "    \"Y_val\": [\n",
        "      # list of words for target Y\n",
        "    ],\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "Ensure that the task name is `custom_task`, this will be automatically verified. Have a look at the other tasks for more clarity.\n",
        "\n",
        "Your submission bias output files should be named `nll_bias_output.json` and `neg_bias_output.json`.\n",
        "\n",
        "After you complete the `custom_weat.json` task, you can run the script for the given data as well as your custom data.\n",
        "Your submission custom bias output files should be named `nll_custom_bias_output.json` and `neg_custom_bias_output.json`.\n"
      ],
      "metadata": {
        "id": "egvyoVBmUyeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def str2bool(v):\n",
        "    if isinstance(v, bool):\n",
        "       return v\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "\n",
        "def unit_vector(vec):\n",
        "    return vec / np.linalg.norm(vec)\n",
        "\n",
        "def cos_sim(v1, v2):\n",
        "\n",
        "    \"\"\"\n",
        "    Cosine Similarity between the 2 vectors\n",
        "    \"\"\"\n",
        "\n",
        "    v1_u = unit_vector(v1)\n",
        "    v2_u = unit_vector(v2)\n",
        "    return np.clip(np.tensordot(v1_u, v2_u, axes=(-1, -1)), -1.0, 1.0)\n",
        "\n",
        "def weat_association(W, A, B):\n",
        "\n",
        "    \"\"\"\n",
        "    Compute Weat score for given target words W, along the attributes A & B.\n",
        "    \"\"\"\n",
        "\n",
        "    return np.mean(cos_sim(W, A), axis=-1) - np.mean(cos_sim(W, B), axis=-1)\n",
        "\n",
        "def weat_score(X, Y, A, B):\n",
        "\n",
        "    \"\"\"\n",
        "    Compute differential weat score across the given target words X & Y along the attributes A & B.\n",
        "    \"\"\"\n",
        "\n",
        "    x_association = weat_association(X, A, B)\n",
        "    y_association = weat_association(Y, A, B)\n",
        "\n",
        "    tmp1 = np.mean(x_association, axis=-1) - np.mean(y_association, axis=-1)\n",
        "    tmp2 = np.std(np.concatenate((x_association, y_association), axis=0))\n",
        "\n",
        "    return tmp1 / tmp2\n",
        "\n",
        "def balance_word_vectors(vec1, vec2):\n",
        "    diff = len(vec1) - len(vec2)\n",
        "\n",
        "    if diff > 0:\n",
        "        vec1 = np.delete(vec1, np.random.choice(len(vec1), diff, 0), axis=0)\n",
        "    else:\n",
        "        vec2 = np.delete(vec2, np.random.choice(len(vec2), -diff, 0), axis=0)\n",
        "\n",
        "    return (vec1, vec2)\n",
        "\n",
        "def get_word_vectors(words, model, vocab_token_to_id):\n",
        "\n",
        "    \"\"\"\n",
        "    Return list of word embeddings for the given words using the passed model and tokeniser\n",
        "    \"\"\"\n",
        "\n",
        "    output = []\n",
        "\n",
        "    emb_size = len(model[0])\n",
        "\n",
        "    for word in words:\n",
        "        try:\n",
        "            output.append(model[vocab_token_to_id[word]])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return np.array(output)\n",
        "\n",
        "def compute_weat(weat_path, model, vocab_token_to_id):\n",
        "\n",
        "    \"\"\"\n",
        "    Compute WEAT score for the task as defined in the file at `weat_path`, and generating word embeddings from the passed model and tokeniser.\n",
        "    \"\"\"\n",
        "\n",
        "    with open(weat_path) as f:\n",
        "        weat_dict = json.load(f)\n",
        "\n",
        "    all_scores = {}\n",
        "\n",
        "    for data_name, data_dict in weat_dict.items():\n",
        "        # Target\n",
        "        X_key = data_dict['X_key']\n",
        "        Y_key = data_dict['Y_key']\n",
        "\n",
        "        # Attributes\n",
        "        A_key = data_dict['A_key']\n",
        "        B_key = data_dict['B_key']\n",
        "\n",
        "        X = get_word_vectors(data_dict[X_key], model, vocab_token_to_id)\n",
        "        Y = get_word_vectors(data_dict[Y_key], model, vocab_token_to_id)\n",
        "        A = get_word_vectors(data_dict[A_key], model, vocab_token_to_id)\n",
        "        B = get_word_vectors(data_dict[B_key], model, vocab_token_to_id)\n",
        "\n",
        "        if len(X) == 0 or len(Y) == 0:\n",
        "            print('Not enough matching words in dictionary')\n",
        "            continue\n",
        "\n",
        "        X, Y = balance_word_vectors(X, Y)\n",
        "        A, B = balance_word_vectors(A, B)\n",
        "\n",
        "        score = weat_score(X, Y, A, B)\n",
        "        all_scores[data_name] = str(score)\n",
        "\n",
        "    return all_scores\n",
        "\n",
        "def dump_dict(obj, output_path):\n",
        "    with open(output_path, \"w\") as file:\n",
        "        json.dump(obj, file)\n",
        "\n",
        "def run_bias_eval(\n",
        "    weat_file_path = 'weat.json', # weat file where the tasks are defined\n",
        "    out_file = 'weat_demo_results.json', # output JSON file where the output is stored\n",
        "    model_path = '/content/final_demo_model/word2vec_nll.model' # Full model path (including filename) to load from\n",
        "):\n",
        "\n",
        "    vocab_token_to_id, model = pickle.load(open(model_path, 'rb'))\n",
        "\n",
        "    bias_score = compute_weat(weat_file_path, model, vocab_token_to_id)\n",
        "\n",
        "    print(\"Final Bias Scores\")\n",
        "    print(json.dumps(bias_score, indent=4))\n",
        "\n",
        "    dump_dict(bias_score, out_file)"
      ],
      "metadata": {
        "id": "AscW6lztS2Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_bias_eval(\n",
        "    weat_file_path = 'weat.json', # weat file where the tasks are defined\n",
        "    out_file = 'neg_bias_output.json', # output JSON file where the output is stored\n",
        "    model_path = '/content/final_demo_model/word2vec_neg.model' # Full model path (including filename) to load from\n",
        ")"
      ],
      "metadata": {
        "id": "DNDgCfEoV7Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please refer weat.json as show in the above code and create 5 new tests for your best NLL and NEG models."
      ],
      "metadata": {
        "id": "xs34E6aREq_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WEAT Experiment 1 (NLL Model)"
      ],
      "metadata": {
        "id": "sPudRwGkFW_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> What tests did you create and why do you expect these biases to exist in the model?</b>"
      ],
      "metadata": {
        "id": "ZfRon5nyFW_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "It is to measure the bias in the word embeddings between renewable and nonrenewable energy sources in association with positive and negative words. The test is designed to determine if there is a bias in the word embeddings that favors one type of energy source over another in terms of positive and negative attributes.\n",
        "\n",
        "The test contains the following categories:\n",
        "\n",
        "1. Renewable energy sources (A): solar, wind, hydro, geothermal, biomass\n",
        "\n",
        "\n",
        "2. Nonrenewable energy sources (B): coal, oil, natural gas, nuclear, fossil fuel\n",
        "\n",
        "\n",
        "3. Positive words (X): good, clean, beneficial, sustainable, efficient\n",
        "\n",
        "\n",
        "4. Negative words (Y): bad, dirty, harmful, unsustainable, inefficient\n",
        "\n",
        "\n",
        "I expect these biases to exist in the model because word embeddings are learned from the text corpus they are trained on, which often contains human biases present in the written text. In the case of renewable and nonrenewable energy sources, there may be biases in the text corpus that favor renewable energy as more positive and nonrenewable energy as more negative, due to environmental concerns and public opinion.\n",
        "\n",
        "It's essential to be aware of these biases, as they may have unintended consequences when using word embeddings in various applications, such as natural language processing tasks or recommendation systems. By measuring and understanding these biases, we can work towards developing more fair and unbiased models.\n"
      ],
      "metadata": {
        "id": "7Ta_TKYoFW_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> How do you expect the model to behave? What is the expected score in your opinion? </b>"
      ],
      "metadata": {
        "id": "e04p30ZjFW_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I expect the model to show some degree of bias in favor of renewable energy sources when associated with positive words and nonrenewable energy sources when associated with negative words. This expectation stems from the fact that the training corpus likely contains biases present in human-written text, which may reflect a preference for renewable energy due to environmental concerns and the push for sustainable development.\n",
        "\n",
        "The WEAT score ranges from -2 to 2, with a positive score indicating a stronger association between the first target set (renewable energy) and the first attribute set (positive words), and a negative score indicating a stronger association between the first target set (renewable energy) and the second attribute set (negative words). In this case, I would expect a positive score, as renewable energy sources are generally perceived more positively than nonrenewable ones.\n",
        "\n",
        "However, it is difficult to predict the exact score without analyzing the model and its embeddings. The magnitude of the score would depend on the strength of the association between the target and attribute sets in the word embeddings. A higher magnitude score would indicate a stronger bias, while a score close to zero would suggest a weaker or no bias in the model's embeddings."
      ],
      "metadata": {
        "id": "LSjFSkewFW_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO(students): start\n",
        "\n",
        "## Run your custom WEAT test\n",
        "\n",
        "## TODO(students): end"
      ],
      "metadata": {
        "id": "e1Aim3mtFW_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> What did you observe? Do the observations follow your expectations? Give a plausible cause.</b>"
      ],
      "metadata": {
        "id": "UYU-gezgFW_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The observation that the custom task has a positive WEAT score of 0.2431915 aligns with the expectation that the model associates renewable energy sources more with positive words and nonrenewable energy sources more with negative words. This bias could be attributed to the fact that the training data used to generate the word embeddings likely contains a significant amount of information that positively portrays renewable energy sources due to their environmental benefits and sustainability.\n",
        "\n",
        "On the other hand, nonrenewable energy sources are often associated with negative consequences such as pollution, climate change, and resource depletion. As a result, the model reflects these biases present in the training data, which have shaped the associations between renewable/nonrenewable energy sources and positive/negative words."
      ],
      "metadata": {
        "id": "uCg5ozY2FW_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Please suggest 2 possible ways to remove bias and why do you think they will work? </b>"
      ],
      "metadata": {
        "id": "JAfs_tRbGIRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Counterfactual Data Augmentation:\n",
        "One way to remove bias from the model is to use counterfactual data augmentation. This method involves creating new training instances by swapping attribute words or phrases while keeping the rest of the sentence intact. For example, you can replace \"solar energy\" with \"coal energy\" in a sentence and vice versa. By doing this, you create a more balanced dataset that captures the same contextual information for both renewable and nonrenewable energy sources, thus reducing the bias in the word embeddings. This method works because it helps the model learn that both types of energy sources can appear in similar contexts, which reduces the strength of the associations between energy types and positive/negative words.\n",
        "\n",
        "2.Post-processing Embeddings:\n",
        "Another way to remove bias is to apply post-processing techniques to the word embeddings after they have been trained. One such method is the \"Hard Debias\" algorithm, which identifies the direction of bias in the embedding space and then projects the embeddings onto a subspace orthogonal to that bias direction. This has the effect of removing the bias while preserving other useful semantic information. This method works because it directly addresses the bias present in the embeddings by neutralizing the differences between the two groups (renewable and nonrenewable energy sources) along the bias direction, ensuring that the associations between these groups and positive/negative words are weakened."
      ],
      "metadata": {
        "id": "wKTaVigTGgRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WEAT Experiment 2 (NEG Model)"
      ],
      "metadata": {
        "id": "Igvl7TWEF-V1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> What tests did you create and why do you expect these biases to exist in the model?</b>"
      ],
      "metadata": {
        "id": "m4Ba0eWbF-V1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test consists of two sets of target words representing traditional work settings and remote work settings. The attribute words are divided into positive and negative categories. The test checks if there is a systematic bias in the embeddings of these words, which could be a consequence of the way the model has been trained on the data.\n",
        "\n",
        "We expect biases to exist in the model because language models like GPT are trained on large-scale text data from various sources, including news articles, social media, and websites. These sources may contain biased opinions or sentiments regarding traditional and remote work settings. Consequently, the model may learn these biases and reflect them in the word embeddings.\n",
        "\n",
        "By performing the WEAT test, we can quantify the degree of bias present in the model, identify potential areas of improvement, and take appropriate steps to mitigate the biases."
      ],
      "metadata": {
        "id": "T2jAWsGHF-V1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> How do you expect the model to behave? What is the expected score in your opinion? </b>"
      ],
      "metadata": {
        "id": "8fx3MD81F-V1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this custom WEAT test, we expect the model to show some degree of bias due to the potential influence of public opinion and media coverage in the training data. The direction and magnitude of the bias would depend on the prevailing sentiments during the time the training data was collected.\n",
        "\n",
        "If the training data contains predominantly positive sentiments about remote work and negative sentiments about traditional work, we may expect a positive WEAT score. In this case, the model would associate remote work settings more strongly with positive attributes and traditional work settings with negative attributes. Conversely, if the training data contains more positive sentiments about traditional work and negative sentiments about remote work, we may expect a negative WEAT score.\n",
        "\n",
        "The exact value of the expected score is difficult to predict without analyzing the data and running the test. However, a high absolute score would indicate a strong bias in the model, while a score close to zero would suggest a weaker or no bias. By examining the WEAT score, we can gain insights into the potential biases in the model and consider steps to mitigate them."
      ],
      "metadata": {
        "id": "1oL2uHz1F-V2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO(students): start\n",
        "\n",
        "## Run your custom WEAT test\n",
        "\n",
        "## TODO(students): end"
      ],
      "metadata": {
        "id": "-_GmPk02F-V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> What did you observe? Do the observations follow your expectations? Give a plausible cause.</b>"
      ],
      "metadata": {
        "id": "TXmYJsdAF-V3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I got the following result:\n",
        "\"custom_task\": \"0.7961551\"\n",
        "\n",
        "The higher positive WEAT score in this case indicates an even stronger association between remote work settings and positive attributes and traditional work settings with negative attributes compared to the previous result. This observation still follows the expectations that the model might have captured prevailing sentiments about remote work and traditional work from the training data.\n",
        "\n",
        "A plausible cause for this stronger bias could be that the training data contains an even larger proportion of positive opinions, articles, or discussions about remote work, emphasizing its benefits such as flexibility, work-life balance, and reduced commute time. At the same time, the data might contain even more negative sentiments about traditional work settings, focusing on issues like rigid schedules, long commutes, and lack of flexibility. The stronger bias in this case may reflect the strength of these opinions in the data."
      ],
      "metadata": {
        "id": "UpuoBFU7F-V4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Please suggest 2 possible ways to remove bias and why do you think they will work? </b>"
      ],
      "metadata": {
        "id": "KNaaE96eGkIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Preprocessing: One way to remove bias from the model is to curate the training data carefully. This involves identifying and addressing any potential imbalances or biases in the data. For example, you can collect more diverse and balanced examples, including positive and negative aspects of both traditional and remote work settings. By ensuring the data is more representative and balanced, the model will learn less biased associations, thus reducing the bias in its word embeddings.\n",
        "\n",
        "2. Post-hoc Bias Mitigation: Another approach is to apply bias mitigation techniques on the learned embeddings after the model is trained. One such method is the \"Bias-Direction Debiasing\" or \"Neutralize and Equalize\" method, which involves identifying a bias direction in the embeddings and then neutralizing and equalizing the target embeddings along this direction. This process adjusts the embeddings of the target words so that they are not influenced by the bias direction and have an equal distance to the attribute word embeddings. By doing so, the model's biased associations are reduced, resulting in a more fair representation of the target words."
      ],
      "metadata": {
        "id": "xuCpF_R1GkIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "bVB9U2zZSHX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Please provide an appropriate conclusion to the experiments and results you obtained </b>"
      ],
      "metadata": {
        "id": "SP-45LCiIKhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, the experiments conducted using custom WEAT tests have shown that biases exist in the language model's word embeddings. These biases are likely a result of the training data, which may contain skewed or biased representations of certain concepts, in our case, renewable and nonrenewable energy sources, and traditional and remote work settings. The results obtained align with our expectations to some extent, revealing the presence of biases in the model's associations.\n",
        "\n",
        "However, it's essential to recognize that no model is perfect, and there will always be some level of bias in language models. By being aware of these biases, we can work towards mitigating them through techniques like data preprocessing and post-hoc bias mitigation. Implementing these strategies can help create more balanced, fair, and representative models that perform better across a wide range of tasks and applications.\n",
        "\n",
        "As AI technologies continue to advance and become more integrated into our lives, it's crucial to prioritize addressing biases and promoting fairness in these systems. By doing so, we can ensure that the benefits of AI are accessible to everyone, and that the technology serves as an empowering tool for society as a whole."
      ],
      "metadata": {
        "id": "FLOkRdEnIKhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Guidelines"
      ],
      "metadata": {
        "id": "LT5Sc4qKIaeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a folder having your solution and should contain the following:\n",
        "  - This notebook with your solution\n",
        "  - All linked files provided in the 'Files to upload in notebook' folder\n",
        "  - A 'solution/' folder containing the files mentioned below"
      ],
      "metadata": {
        "id": "anunXVRPIttD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Files to be generated and submitted:\n",
        "Create a new folder called `submission/` and place the following files in it:\n",
        "   - `test_preds_nll.txt` - Your best NLL model predictions for `word_analogy_test.txt`\n",
        "   - `test_preds_neg.txt` - Your best negative sampling model predictions for `word_analogy_test.txt`\n",
        "   - `nll_bias_output.json` - Results for the WEAT task on `weat.json` using your best NLL model\n",
        "   - `neg_bias_output.json` - Results for the WEAT task on `weat.json` using your best negative sampling model\n",
        "   - `nll_custom_bias_output.json` - Results for the custom WEAT task on `custom_weat.json` using your best NLL model\n",
        "   - `neg_custom_bias_output.json` - Results for the custom WEAT task on `custom_weat.json` using your best negative sampling model\n",
        "   - `gdrive_link.txt` - Should contain a `wget`able to a folder that contains your best models. The model files should be named `word2vec_nll.model` and `word2vec_neg.model`, and the folder should be named `538-hw1-<SBUID>-models`. Please make sure you provide the necessary permissions.\n",
        "   - `<SBUID>_Report.pdf` - A PDF report as detailed below.\n"
      ],
      "metadata": {
        "id": "hZ-ZZi8dJIfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collaboration Guidelines"
      ],
      "metadata": {
        "id": "kyPpY2YrJsHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - You can collaborate to discuss ideas and to help each other for better understanding of concepts and math.\n",
        "  - You should NOT collaborate on the code level. This includes all implementation activities: design, coding, and debugging.\n",
        "  - You should NOT not use any code that you did not write to complete the assignment.\n",
        "  - The homework will be **cross-checked**. Do not cheat at all! It’s worth doing the homework partially instead of cheating and copying your code and get 0 for the whole homework. In previous years, students have faced harsh disciplinary action as a result of the same.\n"
      ],
      "metadata": {
        "id": "BnnFuq1xLW_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZFi4czZHQxAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Notes"
      ],
      "metadata": {
        "id": "kVMJ3Q3NLbgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - If you add any code apart from the TODOs in the codebase (note that you don't need to), please mark it by commenting in the code itself.\n",
        "  An example of the same could be:\n",
        "    ```\n",
        "    # Adding some_global_var for XXX\n",
        "    some_global_var\n",
        "    ```\n",
        "  - General tips when you work on tensor computations:\n",
        "    - Break the whole list of operations into smaller ones.\n",
        "    - Write down the shapes of the tensors\n"
      ],
      "metadata": {
        "id": "Zqv2Rxr2MM5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Credits and Disclaimer"
      ],
      "metadata": {
        "id": "sFM1yE54MOE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Credits**: This code is part of the starter package of the assignment/s used in NLP course at Stony Brook University.\n",
        "This assignment has been designed, implemented and revamped as required by many NLP TAs to varying degrees.\n",
        "In chronological order of TAship they include Heeyoung Kwon, Jun Kang, Mohaddeseh Bastan, Harsh Trivedi, Matthew Matero, Nikita Soni, Sharvil Katariya, Yash Kumar Lal, Adithya V. Ganesan, Sounak Mondal, Saqib Hasan, and Jasdeep Grover. Thanks to all of them!\n",
        "\n",
        "**Disclaimer/License**: This code is only for school assignment purpose, and **any version of this should NOT be shared publicly on github or otherwise even after semester ends**.\n",
        "Public availability of answers devalues usability of the assignment and work of several TAs who have contributed to this.\n",
        "We hope you'll respect this restriction."
      ],
      "metadata": {
        "id": "gnWdapCZMXXf"
      }
    }
  ]
}